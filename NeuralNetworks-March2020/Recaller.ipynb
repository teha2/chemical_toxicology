{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmGwdQappFPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- ChAIkerasRecall.py -*-\n",
        "\"\"\"\n",
        "Created Oct 2019\n",
        "\n",
        "author: Timothy E H Allen / Elena Gelzintye\n",
        "\"\"\"\n",
        "\n",
        "# Import the usual suspects\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# DEFINE INPUTS FOR MODEL RECALL\n",
        "\n",
        "'''\n",
        "receptor = biological target of interest\n",
        "molecule = assessed molecule\n",
        "prediction_data = fingerprints to make predictions on (.csv)\n",
        "prediction_output_location = file to write predictions to (.csv)\n",
        "training_data = oringinal training data for similarity calculations (.csv)\n",
        "model_location = model file (.h5)\n",
        "network_activation_strings = location to put activation strings for prediction compounds\n",
        "network_activation_strings_training = location to put activation strings for training compounds\n",
        "similarity_output_location = location to put similarity outputs\n",
        "'''\n",
        "\n",
        "receptor = \"AR\"\n",
        "molecule = \"Andarine\"\n",
        "prediction_data = \"/content/drive/My Drive/\" + molecule + \" fingerprints ECFP4 10000.csv\"\n",
        "training_data = \"/content/drive/My Drive/\" + receptor + \" fingerprints ECFP4 10000.csv\"\n",
        "prediction_output_location = \"/content/drive/My Drive/\" + molecule + \"_Prediction.csv\"\n",
        "model_location = \"/content/drive/My Drive/\" + receptor + \" model.h5\"\n",
        "network_activation_strings = \"/content/drive/My Drive/\" + molecule + \" test_NAS\"\n",
        "network_activation_strings_training = \"/content/drive/My Drive/\" + molecule + \" training_NAS\"\n",
        "similarity_output_location = \"/content/drive/My Drive/\" + molecule + \"_Similarity_Out\"\n",
        "\n",
        "print(\"Welcome to ChAI\")\n",
        "print(\"Dataset loading...\")\n",
        "\n",
        "# Reading the prediction dataset\n",
        "\n",
        "def read_dataset(input_data):\n",
        "    df = pd.read_csv(input_data)\n",
        "    X = df[df.columns[0:10000]].values\n",
        "    y = df[df.columns[10000]]\n",
        "\n",
        "    # Encode the dependent variable\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    Y = encoder.transform(y)\n",
        "    print(\"X.shape =\", X.shape)\n",
        "    print(\"Y.shape =\", Y.shape)\n",
        "    print(\"y.shape =\", y.shape)\n",
        "    return (X, Y)\n",
        "\n",
        "# Define the encoder function\n",
        "\n",
        "def one_hot_encode(labels):\n",
        "    n_labels = len(labels)\n",
        "    n_unique_labels = len(np.unique(labels))\n",
        "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
        "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
        "    return one_hot_encode\n",
        "\n",
        "X, Y = read_dataset(prediction_data)\n",
        "A, B = read_dataset(training_data)\n",
        "\n",
        "n_compounds = X.shape[0]\n",
        "n_dim = X.shape[1]\n",
        "n_class = 2\n",
        "\n",
        "print(\"Input\", n_compounds, \"compounds for prediction\")\n",
        "\n",
        "f = open(prediction_output_location, 'w+')\n",
        "\n",
        "# Call and use pretrained model\n",
        "\n",
        "pretrained_model = tf.keras.models.load_model(model_location)\n",
        "pretrained_model.summary()\n",
        "y = pretrained_model.predict(X, verbose=1)\n",
        "\n",
        "y_ten = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "y1_ten = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    prediction = sess.run(tf.argmax(y_ten, 1))\n",
        "    accuracy = sess.run(tf.cast(tf.equal(tf.argmax(y_ten, 1), (y1_ten).eval()), tf.float32))\n",
        "    total_accuracy = str(sess.run(tf.reduce_mean(accuracy)))\n",
        "    print((y_ten).eval(),file=f)\n",
        "    print((y1_ten).eval(),file=f)\n",
        "    print(Y,file=f)\n",
        "\n",
        "original_class = Y\n",
        "print(\"Overall Model Accuracy = \" + total_accuracy)\n",
        "\n",
        "print(\"Model Recall Initialized\")\n",
        "\n",
        "print(\"**************************************************\")\n",
        "print(\"0 stands for miss & 1 stands for hit at the target\")\n",
        "print(\"**************************************************\")\n",
        "i=0\n",
        "\n",
        "for i in range(0,n_compounds):\n",
        "#    print(\"Original Class : \", original_class[i], \"Predicted Values : \", prediction[i], \"Accuracy : \", accuracy[i], \"Probability Active : \", y[i,1])\n",
        "    print(\"Original Class,\", original_class[i], \",Predicted Values,\", prediction[i], \",Accuracy,\", accuracy[i], \",Probability Active,\", y[i,1] , file=f)\n",
        "\n",
        "f.close()\n",
        "\n",
        "#%%\n",
        "print('commencing similarity calculation')\n",
        "\n",
        "# Reading the prediction dataset\n",
        "\n",
        "receptor='target'\n",
        "method='method'\n",
        "fold=1\n",
        "\n",
        "# Define functions for similarity calculations\n",
        "\n",
        "def layer_propagation(x, weight, bias, function):\n",
        "    \n",
        "#    with given x (from previous layer or from input) and given weights and biases\n",
        "#    propagates one layer. NB tensor dimensions must match\n",
        "    \n",
        "    layer=tf.add(tf.matmul(x, weight), bias)\n",
        "    if function=='relu':\n",
        "        layer=tf.nn.relu(layer)\n",
        "    elif function=='sigmoid':\n",
        "        layer=tf.nn.sigmoid(layer)    \n",
        "    \n",
        "    return layer\n",
        "\n",
        "def get_euclidean_dist_mx(string1, string2):\n",
        "    similarities=np.zeros((len(string1), len(string2)))\n",
        "    for no_1 in range(0, len(string1)):\n",
        "        for no_2 in range(0, len(string2)): \n",
        "            \n",
        "            similarities[no_1, no_2] = distance.euclidean(string1[no_1],string2[no_2])\n",
        "            \n",
        "    return similarities\n",
        "\n",
        "# Get weights and biases from keras model\n",
        "    \n",
        "no_hidden_layers = len(pretrained_model.layers) - 2\n",
        "\n",
        "if no_hidden_layers == 1:\n",
        "    weights_h1 = pretrained_model.layers[1].get_weights()[0]\n",
        "    biases_b1 = pretrained_model.layers[1].get_weights()[1]\n",
        "    \n",
        "elif no_hidden_layers == 2:\n",
        "    weights_h1 = pretrained_model.layers[1].get_weights()[0]\n",
        "    biases_b1 = pretrained_model.layers[1].get_weights()[1]\n",
        "    weights_h2 = pretrained_model.layers[2].get_weights()[0]\n",
        "    biases_b2 = pretrained_model.layers[2].get_weights()[1]\n",
        "\n",
        "else:\n",
        "    weights_h1 = pretrained_model.layers[1].get_weights()[0]\n",
        "    biases_b1 = pretrained_model.layers[1].get_weights()[1]\n",
        "    weights_h2 = pretrained_model.layers[2].get_weights()[0]\n",
        "    biases_b2 = pretrained_model.layers[2].get_weights()[1]\n",
        "    weights_h3 = pretrained_model.layers[3].get_weights()[0]\n",
        "    biases_b3 = pretrained_model.layers[3].get_weights()[1]\n",
        "\n",
        "# Get network activation strings\n",
        "\n",
        "X_val=X.astype(np.float32)\n",
        "A_val=A.astype(np.float32)\n",
        "\n",
        "n_samples=len(X_val)\n",
        "\n",
        "len_fp=X_val.shape[1] \n",
        "\n",
        "# Get strings for validation compounds\n",
        "   \n",
        "fingerprint = tf.placeholder(tf.float32, [None, len_fp], name='fp_placehold')\n",
        "\n",
        "if no_hidden_layers == 1:\n",
        "        activ_funs=('sigmoid')\n",
        "        A1=layer_propagation(X_val, weights_h1, biases_b1, activ_funs[0])\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            A1=sess.run(A1)\n",
        "\n",
        "        joint=A1\n",
        "    \n",
        "elif no_hidden_layers == 2:\n",
        "        activ_funs=('sigmoid', 'relu')\n",
        "        A1=layer_propagation(X_val, weights_h1, biases_b1, activ_funs[0])\n",
        "        A2=layer_propagation(A1, weights_h2, biases_b2, activ_funs[1])\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            A1=sess.run(A1)\n",
        "            A2=sess.run(A2)\n",
        "\n",
        "        joint_temp=np.concatenate(([A1], [A2]), axis=2)\n",
        "        joint = np.squeeze(joint_temp, axis = 0)\n",
        "\n",
        "else:\n",
        "        activ_funs=('sigmoid', 'sigmoid', 'relu')\n",
        "        A1=layer_propagation(X_val, weights_h1, biases_b1, activ_funs[0])\n",
        "        A2=layer_propagation(A1, weights_h2, biases_b2, activ_funs[1])\n",
        "        A3=layer_propagation(A2, weights_h3, biases_b3, activ_funs[2])\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            A1=sess.run(A1)\n",
        "            A2=sess.run(A2)\n",
        "            A3=sess.run(A3)\n",
        "\n",
        "        joint_temp=np.concatenate(([A1], [A2], [A3]), axis=2)\n",
        "        joint = np.squeeze(joint_temp, axis = 0)\n",
        "    \n",
        "print(joint.shape)\n",
        "print(type(joint))\n",
        "print(joint)\n",
        "\n",
        "np.save(network_activation_strings.format(receptor, method, fold), joint)\n",
        "\n",
        "# Get strings for training compounds\n",
        "   \n",
        "fingerprint_train = tf.placeholder(tf.float32, [None, len_fp], name='fp_placehold')\n",
        "\n",
        "if no_hidden_layers == 1:\n",
        "        A1_train=layer_propagation(A_val, weights_h1, biases_b1, activ_funs[0])\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            A1_train=sess.run(A1_train)\n",
        "\n",
        "        joint_train=A1_train\n",
        "    \n",
        "elif no_hidden_layers == 2:\n",
        "        A1_train=layer_propagation(A_val, weights_h1, biases_b1, activ_funs[0])\n",
        "        A2_train=layer_propagation(A1_train, weights_h2, biases_b2, activ_funs[1])\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            A1_train=sess.run(A1_train)\n",
        "            A2_train=sess.run(A2_train)\n",
        "\n",
        "        joint_train_temp=np.concatenate(([A1_train], [A2_train]), axis=2)\n",
        "        joint_train = np.squeeze(joint_train_temp, axis = 0)\n",
        "\n",
        "else:\n",
        "        A1_train=layer_propagation(A_val, weights_h1, biases_b1, activ_funs[0])\n",
        "        A2_train=layer_propagation(A1_train, weights_h2, biases_b2, activ_funs[1])\n",
        "        A3_train=layer_propagation(A2_train, weights_h3, biases_b2, activ_funs[2])\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            A1_train=sess.run(A1_train)\n",
        "            A2_train=sess.run(A2_train)\n",
        "            A3_train=sess.run(A3_train)\n",
        "\n",
        "        joint_train_temp=np.concatenate(([A1_train], [A2_train], [A3_train]), axis=2)\n",
        "        joint_train = np.squeeze(joint_train_temp, axis = 0)\n",
        "\n",
        "print(joint_train.shape)\n",
        "print(type(joint_train))\n",
        "print(joint_train)\n",
        "\n",
        "np.save(network_activation_strings_training.format(receptor, method, fold), joint)\n",
        "\n",
        "#generate similarity matrix - takes a while\n",
        "print(\"Generating similarity matrix\")\n",
        "ntw_eucl_dist=get_euclidean_dist_mx(joint, joint_train)\n",
        "ntw_eucl_sim=1/(1+ntw_eucl_dist)\n",
        "\n",
        "np.save(similarity_output_location + \".npy\".format(receptor, method, fold), ntw_eucl_sim)\n",
        "\n",
        "ntw_eucl_sim=np.load(similarity_output_location + \".npy\".format(receptor, method, fold))\n",
        "\n",
        "#convert to dataframe, save as csv\n",
        "print(\"Saving dataframe as CSV\")\n",
        "ntw_eucl_sim_pandas=pd.DataFrame(ntw_eucl_sim)\n",
        "ntw_eucl_sim_pandas.to_csv(similarity_output_location + \".csv\".format(receptor, method, fold))\n",
        "\n",
        "#Endgame\n",
        "print(\"END\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}