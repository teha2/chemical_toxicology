{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clusterer.ipynb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTg-uznl45c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- ClusterGenerator.py -*-\n",
        "\"\"\"\n",
        "Created Feb 2019\n",
        "\n",
        "@author: Elena Gelzintye / Timothy E H Allen\n",
        "\n",
        "Clusters dataset into equal parts with similarity less than x between the clusters\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import random\n",
        "from rdkit import Chem\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import squareform \n",
        "from collections import Counter\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "\n",
        "def get_fingerprint(smiles):\n",
        "    '''generates fingerprint dataframe given the smiles dataframe with 'SMILES' column containing the smiles. Also saves bit information needed for recovering the substructures later'''\n",
        "    \n",
        "    bit_infos=[]\n",
        "    rdkit_molecules=[Chem.MolFromSmiles(x) for x in smiles['SMILES']]\n",
        "    rdkit_fingerprint=[]\n",
        "    for mol in rdkit_molecules:\n",
        "        bit_info={}\n",
        "        fp=rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=4, nBits=10000, \\\n",
        "                                                                      bitInfo=bit_info).ToBitString() \n",
        "        bit_infos.append(bit_info)\n",
        "        rdkit_fingerprint.append(fp)        \n",
        "    fingerprint_df=pd.DataFrame([np.array(list(x)).astype(int) for x in rdkit_fingerprint])\n",
        "    return fingerprint_df, bit_infos\n",
        "\n",
        "def Tanimoto (fp1, fp2):\n",
        "    if len(fp1)!=len(fp2):\n",
        "        print('fingerprint lengths do not match')\n",
        "        return None\n",
        "    added=np.add(fp1,fp2)\n",
        "    double_on=np.count_nonzero(added==2)           \n",
        "    total_on=np.count_nonzero(added)\n",
        "    TMS=double_on/total_on\n",
        "    return TMS\n",
        "    \n",
        "def get_similarities(dataframe):\n",
        "    \n",
        "#    returns 2d numpy similarity array given the dataframe of fingerprints\n",
        "        \n",
        "    similarities=np.zeros((dataframe.shape[0], dataframe.shape[0]))\n",
        "    fingerprints=np.array(dataframe)\n",
        "    \n",
        "    for no_1 in range(0, len(fingerprints)):\n",
        "        for no_2 in range(no_1, len(fingerprints)):\n",
        "            \n",
        "            similarities[no_1, no_2]=Tanimoto(fingerprints[no_1], fingerprints[no_2])\n",
        "\n",
        "    similarities=similarities+similarities.T-np.diag(similarities.diagonal())\n",
        "    \n",
        "    return similarities\n",
        "\n",
        "class data_point:\n",
        "    def __init__(self, name, position, cluster=None, cluster_size=None):\n",
        "        self.name=name\n",
        "        self.position=position\n",
        "        self.cluster=cluster\n",
        "        self.cluster_size=cluster_size\n",
        "    def print_stuff(self):\n",
        "        print('{}: {}'.format(self.name, self.position))\n",
        "\n",
        "receptor = \"AR\"\n",
        "smiles=pd.read_csv(\"/content/drive/My Drive/\" + receptor + \".csv\".format(receptor,receptor))\n",
        "\n",
        "#generate fingeprint \n",
        "fingerprint, bit_infos=get_fingerprint(smiles) \n",
        "fingerprint=np.array(fingerprint)\n",
        "smiles=np.array(smiles)\n",
        "\n",
        "#calculate tanimoto similarity matrix\n",
        "similarities=get_similarities(fingerprint)\n",
        "\n",
        "\n",
        "#generate distance matrix \n",
        "distances=np.zeros((len(similarities), len(similarities)))\n",
        "for no1 in range(len(similarities)):\n",
        "    for no2 in range(len(similarities)):\n",
        "        distances[no1][no2]=1-similarities[no1][no2]\n",
        "        \n",
        "\n",
        "#Do the hierarchical clustering\n",
        "#Hierarchy-cluster into clusters and set cut-off \n",
        "\n",
        "distances_condensed=squareform(distances)\n",
        "Z=linkage(distances_condensed, method='single')\n",
        "\n",
        "#%%\n",
        "#for 5-fold cross validation\n",
        "no_clusters=5\n",
        "#arbitrarily set the largest cluster size and maximum distance between two clusters (yields more than 5 clusters)\n",
        "largest_cluster=len(fingerprint)\n",
        "max_d=0.3\n",
        "#iterate until the largest cluster is no larger than half of the cross-validation group by allowing increasingly lower max distance between clusters/higher maximum similarity between the clusters. \n",
        "while largest_cluster>int(len(fingerprint)/(no_clusters*2)):\n",
        "    \n",
        "    max_d=max_d*0.99\n",
        "    \n",
        "    clusters = fcluster(Z, max_d, criterion='distance')\n",
        "    clusters=list(clusters)\n",
        "    largest_cluster=Counter(clusters).most_common(1)[0][1]\n",
        "    print(max_d, largest_cluster)\n",
        "    \n",
        "\n",
        "\n",
        "counted=Counter(clusters)\n",
        "#make up a class for each compound/point including its id (here 'no') in the smiles list, fingerprint, cluster_id and the number of compounds in the corresponding cluster. \n",
        "all_pts=[data_point(no, fingerprint[no], clust_id, counted[clust_id]) for no, clust_id in enumerate(clusters)]\n",
        "#sort all points based on the size of the cluster they belong to \n",
        "all_pts.sort(key=lambda s: s.cluster_size, reverse=True)\n",
        "\n",
        "#%%\n",
        "# group into 5 groups\n",
        "#initialise with attributing five biggest clusters to different CV groups\n",
        "\n",
        "#dictionary matching cluster_id to the list of compounds (class) belonging to that cluster\n",
        "all_clusters={}\n",
        "for pt in all_pts:\n",
        "    try:\n",
        "        all_clusters[pt.cluster].append(pt)\n",
        "    except KeyError:\n",
        "        all_clusters[pt.cluster]=[pt]\n",
        "\n",
        "#get the keys with decreasing cluster size\n",
        "#get number of points (lengths) and cluster_id in each cluster\n",
        "lengths=[(len(all_clusters[c]), c) for c in all_clusters]\n",
        "#sort cluster_ids based on how many compounds are in each\n",
        "lengths.sort(key= lambda s: s[0], reverse=True)\n",
        "#get list of cluster ids, sorted from largest to smallest\n",
        "keys_ordered=[s[1] for s in lengths]\n",
        "\n",
        "print(keys_ordered)\n",
        "\n",
        "#ideal size of a 5-CV cluster\n",
        "ideal_size=(int(len(fingerprint)/5)) + 1\n",
        "\n",
        "#assign the largest 5 clusters to 5 different cross-validation groups\n",
        "folds={0:all_clusters[keys_ordered[0]], \n",
        "       1:all_clusters[keys_ordered[1]], \n",
        "       2:all_clusters[keys_ordered[2]], \n",
        "       3:all_clusters[keys_ordered[3]],\n",
        "       4:all_clusters[keys_ordered[4]]}\n",
        "#%%\n",
        "#select remaining cluster ids\n",
        "keys_ordered=keys_ordered[5:]\n",
        "\n",
        "keys_left=[]\n",
        "fold_nos=[0,1,2,3,4]\n",
        "for key in keys_ordered: \n",
        "    \n",
        "    #select the compounds in selected cluster to append\n",
        "    to_add=all_clusters[key]\n",
        "    has_been_added=False\n",
        "    \n",
        "    #choose random sequence of CV groups to cycle over\n",
        "    random.shuffle(fold_nos)\n",
        "    for fold in fold_nos:\n",
        "        #add the cluster if there's space in CV group and exit the cycle\n",
        "        if len(folds[fold])+len(to_add)<ideal_size:\n",
        "            folds[fold]=folds[fold]+(to_add)\n",
        "            has_been_added=True\n",
        "            \n",
        "            break\n",
        "    #note which compounds were excluded\n",
        "    if has_been_added==False:\n",
        "        keys_left.append(key)\n",
        "        \n",
        "fold_a = [a.name for a in folds[0]]\n",
        "fold_b = [a.name for a in folds[1]]\n",
        "fold_c = [a.name for a in folds[2]]\n",
        "fold_d = [a.name for a in folds[3]]\n",
        "fold_e = [a.name for a in folds[4]]\n",
        "\n",
        "fingerprint_fold_a = pd.DataFrame(fingerprint[fold_a])\n",
        "fingerprint_fold_b = pd.DataFrame(fingerprint[fold_b])\n",
        "fingerprint_fold_c = pd.DataFrame(fingerprint[fold_c])\n",
        "fingerprint_fold_d = pd.DataFrame(fingerprint[fold_d])\n",
        "fingerprint_fold_e = pd.DataFrame(fingerprint[fold_e])\n",
        "\n",
        "smiles_fold_a = pd.DataFrame(smiles[fold_a])\n",
        "smiles_fold_a = smiles_fold_a.drop([0], axis = 1)\n",
        "smiles_fold_b = pd.DataFrame(smiles[fold_b])\n",
        "smiles_fold_b = smiles_fold_b.drop([0], axis = 1)\n",
        "smiles_fold_c = pd.DataFrame(smiles[fold_c])\n",
        "smiles_fold_c = smiles_fold_c.drop([0], axis = 1)\n",
        "smiles_fold_d = pd.DataFrame(smiles[fold_d])\n",
        "smiles_fold_d = smiles_fold_d.drop([0], axis = 1)\n",
        "smiles_fold_e = pd.DataFrame(smiles[fold_e])\n",
        "smiles_fold_e = smiles_fold_e.drop([0], axis = 1)\n",
        "\n",
        "fingerprint_fold_a = pd.concat([fingerprint_fold_a,smiles_fold_a], axis=1)\n",
        "fingerprint_fold_b = pd.concat([fingerprint_fold_b,smiles_fold_b], axis=1)\n",
        "fingerprint_fold_c = pd.concat([fingerprint_fold_c,smiles_fold_c], axis=1)\n",
        "fingerprint_fold_d = pd.concat([fingerprint_fold_d,smiles_fold_d], axis=1)\n",
        "fingerprint_fold_e = pd.concat([fingerprint_fold_e,smiles_fold_e], axis=1)\n",
        "\n",
        "pd.DataFrame(fingerprint_fold_a).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_a fingerprint ECFP4 10000.csv\", index = False)\n",
        "pd.DataFrame(fingerprint_fold_b).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_b fingerprint ECFP4 10000.csv\", index = False)\n",
        "pd.DataFrame(fingerprint_fold_c).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_c fingerprint ECFP4 10000.csv\", index = False)\n",
        "pd.DataFrame(fingerprint_fold_d).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_d fingerprint ECFP4 10000.csv\", index = False)\n",
        "pd.DataFrame(fingerprint_fold_e).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_e fingerprint ECFP4 10000.csv\", index = False)\n",
        "\n",
        "pd.DataFrame(smiles[fold_a]).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_a.csv\", index = False)\n",
        "pd.DataFrame(smiles[fold_b]).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_b.csv\", index = False)\n",
        "pd.DataFrame(smiles[fold_c]).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_c.csv\", index = False)\n",
        "pd.DataFrame(smiles[fold_d]).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_d.csv\", index = False)\n",
        "pd.DataFrame(smiles[fold_e]).to_csv(\"/content/drive/My Drive/clustered_data/\" + receptor +\" fold_e.csv\", index = False)\n",
        "\n",
        "\n",
        "        \n",
        "##extract the compound indices to later be passed on for \n",
        "indices=[]\n",
        "#cycle over 5 combinations of validation + training indices\n",
        "for k in range(len(folds)):\n",
        "    \n",
        "    #a bunch of indices for validation\n",
        "    #a.name extracts index out of the bundled up data_point class\n",
        "    val_idx=[a.name for a in folds[k]]\n",
        "    \n",
        "    train_idx=[]\n",
        "    for i in range(len(folds)):\n",
        "        #excludes the CV group that was selected for validation\n",
        "        if i!=k:\n",
        "            train_idx=train_idx+[a.name for a in folds[i]]\n",
        "            \n",
        "    indices.append((train_idx,val_idx))\n",
        "    \n",
        "print(train_idx)\n",
        "print(val_idx)\n",
        "\n",
        "#%% \n",
        "\n",
        "indices=np.array(indices)\n",
        "np.save('{} train test split'.format(receptor), indices)\n",
        "pd.DataFrame(np.array(indices)).to_csv(\"AChE train test split.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}