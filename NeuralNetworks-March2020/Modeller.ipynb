{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modeller.ipynb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTg-uznl45c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- ChAIkeras.py -*-\n",
        "\"\"\"\n",
        "Created Oct 2019\n",
        "\n",
        "author: Timothy E H Allen\n",
        "\"\"\"\n",
        "#%%\n",
        "\n",
        "# Import the usual suspects\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "import random\n",
        "\n",
        "# DEFINE INPUTS FOR MODEL TRAINING\n",
        "\n",
        "'''\n",
        "receptor = biological target\n",
        "input_data = clustered datasets\n",
        "rng_1 and rng_2 = random numbers for dataset shuffle and train/validation split\n",
        "validation_proportion = fraction of data to be used as validation set\n",
        "beta = l2 regularisation rate\n",
        "neurons = neurons per hidden layer\n",
        "hidden_layers = number of hidden layers, must be 1, 2 or 3\n",
        "LR = learning rate\n",
        "epochs = number of training iterations\n",
        "model_path = location to save model file post training\n",
        "'''\n",
        "\n",
        "receptor = \"AR\"\n",
        "input_data_a = \"/content/drive/My Drive/Andy_data/clustered_data/\" + receptor + \" fold_a fingerprint ECFP4 10000.csv\"\n",
        "input_data_b = \"/content/drive/My Drive/Andy_data/clustered_data/\" + receptor + \" fold_b fingerprint ECFP4 10000.csv\"\n",
        "input_data_c = \"/content/drive/My Drive/Andy_data/clustered_data/\" + receptor + \" fold_c fingerprint ECFP4 10000.csv\"\n",
        "input_data_d = \"/content/drive/My Drive/Andy_data/clustered_data/\" + receptor + \" fold_d fingerprint ECFP4 10000.csv\"\n",
        "input_data_e = \"/content/drive/My Drive/Andy_data/clustered_data/\" + receptor + \" fold_e fingerprint ECFP4 10000.csv\"\n",
        "rng_1 = random.randrange(1,1000)\n",
        "rng_2 = random.randrange(1,1000)\n",
        "validation_proportion = 0.25\n",
        "beta = 0.1\n",
        "neurons = 100\n",
        "hidden_layers = 2\n",
        "LR = 0.001\n",
        "epochs = 100\n",
        "model_path = \"/content/drive/My Drive/Andy_data/\" + receptor + \" model.h5\"\n",
        "\n",
        "print(\"Welcome to ChAI\")\n",
        "print(\"Dataset loading...\")\n",
        "\n",
        "# Reading The Dataset\n",
        "\n",
        "def read_dataset(input_data):\n",
        "    df = pd.read_csv(input_data)\n",
        "    X = df[df.columns[0:10000]].values\n",
        "    y = df[df.columns[10000]]\n",
        "\n",
        "    # Encode the dependent variable\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    Y = encoder.transform(y)\n",
        "    print(\"X.shape =\", X.shape)\n",
        "    print(\"Y.shape =\", Y.shape)\n",
        "    print(\"y.shape =\", y.shape)\n",
        "    return (X, Y)\n",
        "\n",
        "Xa, Ya = read_dataset(input_data_a)\n",
        "Xb, Yb = read_dataset(input_data_b)\n",
        "Xc, Yc = read_dataset(input_data_c)\n",
        "Xd, Yd = read_dataset(input_data_d)\n",
        "Xe, Ye = read_dataset(input_data_e)\n",
        "\n",
        "X = np.concatenate((Xb,Xc,Xd,Xe))\n",
        "Y = np.concatenate((Yb,Yc,Yd,Ye))\n",
        "\n",
        "\n",
        "# Shuffle the dataset\n",
        " \n",
        "X, Y = shuffle(X, Y, random_state=rng_1)\n",
        "\n",
        "# Convert the dataset into train and validation sets\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(X, Y, test_size =validation_proportion, random_state=rng_2)\n",
        "test_x = Xa\n",
        "test_y = Ya\n",
        "\n",
        "# Inspect the shape of the training and validation data\n",
        "\n",
        "print(\"Dimensionality of data:\")\n",
        "print(\"Train x shape =\", train_x.shape)\n",
        "print(\"Train y shape =\", train_y.shape)\n",
        "print(\"Validation x shape =\", valid_x.shape)\n",
        "print(\"Validation y shape =\", valid_y.shape)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_y),\n",
        "                                                 train_y)\n",
        "\n",
        "#Define the model in keras\n",
        "\n",
        "print(\"Constructing model architecture\")\n",
        "\n",
        "if hidden_layers == 1:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "elif hidden_layers == 2:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "elif hidden_layers == 3:\n",
        "    inputs = keras.Input(shape=(10000,), name='digits')\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_1')(inputs)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_2')(x)\n",
        "    x = layers.Dense(neurons, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(beta), name='dense_3')(x)\n",
        "    outputs = layers.Dense(2, activation='softmax', name='predictions')(x)\n",
        "else:\n",
        "    print(\"Number of hidden layers outside this model scope, please choose 1, 2 or 3\")\n",
        "\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=LR),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "print('Commencing model training...')\n",
        "history = model.fit(train_x, train_y,\n",
        "                    batch_size=128,\n",
        "                    epochs=epochs,\n",
        "                    class_weight=class_weights,\n",
        "                    # We pass some validation for\n",
        "                    # monitoring validation loss and metrics\n",
        "                    # at the end of each epoch\n",
        "                    validation_data=(valid_x, valid_y))\n",
        "\n",
        "# The returned \"history\" object holds a record\n",
        "# of the loss values and metric values during training\n",
        "\n",
        "# Evaluate the model on the training and validation data\n",
        "print('\\n# Evaluate on training data')\n",
        "train_results = model.evaluate(train_x, train_y, batch_size=128)\n",
        "print('train loss, train acc:', train_results)\n",
        "\n",
        "print('\\n# Evaluate on validaiton data')\n",
        "validation_results = model.evaluate(valid_x, valid_y, batch_size=128)\n",
        "print('validation loss, validation acc:', validation_results)\n",
        "\n",
        "# Save the model\n",
        "\n",
        "model.save(model_path)\n",
        "print('Model saved to ' + model_path)\n",
        "\n",
        "pred_valid_y = model.predict(valid_x, verbose=1)\n",
        "pred_train_y = model.predict(train_x, verbose=1)\n",
        "pred_test_y = model.predict(test_x, verbose=1)\n",
        "\n",
        "# Plot history of loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot history of accuracy values\n",
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Define experimental and predicted values using argmax\n",
        "\n",
        "pred_train_y_binary = np.argmax(pred_train_y, axis=1)\n",
        "pred_valid_y_binary = np.argmax(pred_valid_y, axis=1)\n",
        "pred_test_y_binary = np.argmax(pred_test_y, axis=1)\n",
        "\n",
        "# Calculate and display confusion matricies\n",
        "\n",
        "cm = confusion_matrix(train_y, pred_train_y_binary)\n",
        "np.set_printoptions(precision=2)\n",
        "print(\"Confusion matrix (Training), without normalisation\")\n",
        "print(cm)\n",
        "\n",
        "cm = confusion_matrix(valid_y, pred_valid_y_binary)\n",
        "np.set_printoptions(precision=2)\n",
        "print(\"Confusion matrix (Validation), without normalisation\")\n",
        "print(cm)\n",
        "\n",
        "cm = confusion_matrix(test_y, pred_test_y_binary)\n",
        "np.set_printoptions(precision=2)\n",
        "print(\"Confusion matrix (Test), without normalisation\")\n",
        "print(cm)\n",
        "\n",
        "# Attempt a ROC curve\n",
        "def plot_roc(pred,y):\n",
        "    fpr, tpr, _ = roc_curve(y,pred)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
        "    plt.plot([0,1], [0,1], \"k--\")\n",
        "    plt.xlim([0.0,1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# First plot Training data ROC\n",
        "print(\"Training data ROC Curve\")\n",
        "    \n",
        "y_score = np.array(pred_train_y)[:,1]\n",
        "y_true = np.array(train_y)\n",
        "plot_roc(y_score, y_true)\n",
        "\n",
        "# Then plot Validation data ROC\n",
        "print(\"Validation data ROC Curve\")\n",
        "\n",
        "y_score_2 = np.array(pred_valid_y)[:,1]\n",
        "y_true_2 = np.array(valid_y)\n",
        "plot_roc(y_score_2, y_true_2)\n",
        "\n",
        "# Finally plot Test data ROC\n",
        "print(\"Test data ROC Curve\")\n",
        "\n",
        "y_score_3 = np.array(pred_test_y)[:,1]\n",
        "y_true_3 = np.array(test_y)\n",
        "plot_roc(y_score_3, y_true_3)\n",
        "\n",
        "#End the cycle\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "#End the cycle\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "print(\"END\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}